import json
import logging
import time
import cv2
import numpy as np
import threading
from queue import Queue, Empty
from concurrent.futures import ThreadPoolExecutor

from kafka import KafkaConsumer as KafkaPyConsumer
from kafka import KafkaProducer as KafkaPyProducer
from utils.time_utils import timestamp_to_vietnam_time
from utils.minio_client import MinioClient
from config import (
    MINIO_ENDPOINT, MINIO_ACCESS_KEY, MINIO_SECRET_KEY, MINIO_BUCKET_NAME,
    YOLO_MODEL_PATH, IMAGE_BASE_URL,
    BATCH_SIZE, BATCH_TIMEOUT, DOWNLOAD_WORKERS, UPLOAD_WORKERS
)

logger = logging.getLogger(__name__)

class TrafficAnalysisService:
    def __init__(self, kafka_broker, input_topic, output_topic):
        # 1. Khá»Ÿi táº¡o Kafka Consumer (Consumer Group)
        self.consumer = KafkaPyConsumer(
            input_topic,
            bootstrap_servers=kafka_broker,
            auto_offset_reset='latest',
            enable_auto_commit=True,
            group_id='traffic-analysis-group', 
            value_deserializer=lambda x: json.loads(x.decode('utf-8')),
            max_poll_records=10,
            max_poll_interval_ms=300000 # 5 phÃºt
        )
        
        # 2. Khá»Ÿi táº¡o Kafka Producer
        self.producer = KafkaPyProducer(
            bootstrap_servers=kafka_broker,
            value_serializer=lambda x: json.dumps(x).encode('utf-8')
        )

        # 3. Khá»Ÿi táº¡o MinIO & Model
        self.minio_client = MinioClient(MINIO_ENDPOINT, MINIO_ACCESS_KEY, MINIO_SECRET_KEY, MINIO_BUCKET_NAME)
        
        from models.yolo import YOLOModel
        from processing.image_fetcher import ImageFetcher
        from processing.vehicle_counter import VehicleCounter
        
        logger.info(f"Loading Model Batch Size: {BATCH_SIZE}...")
        self.yolo_model = YOLOModel(YOLO_MODEL_PATH)
        self.image_fetcher = ImageFetcher(IMAGE_BASE_URL)
        self.output_topic = output_topic

        # 4. CÃ¡c hÃ ng Ä‘á»£i (Queues) cho Pipeline
        self.inference_queue = Queue(maxsize=BATCH_SIZE * 4) 
        self.upload_queue = Queue(maxsize=BATCH_SIZE * 4)    
        
        # 5. Thread Pools
        self.download_pool = ThreadPoolExecutor(max_workers=DOWNLOAD_WORKERS)
        self.upload_pool = ThreadPoolExecutor(max_workers=UPLOAD_WORKERS)
        
        self.is_running = True

    def start(self):
        logger.info("Starting Traffic Analysis Service")
        
        kafka_thread = threading.Thread(target=self.consume_loop)
        kafka_thread.daemon = True
        kafka_thread.start()

        gpu_thread = threading.Thread(target=self.gpu_inference_loop)
        gpu_thread.daemon = True
        gpu_thread.start()

        result_thread = threading.Thread(target=self.result_processing_loop)
        result_thread.daemon = True
        result_thread.start()

        try:
            while True: time.sleep(1)
        except KeyboardInterrupt:
            self.is_running = False
            logger.info("Stopping service...")

    def consume_loop(self):
        """Luá»“ng Ä‘á»c Kafka vÃ  submit task táº£i áº£nh"""
        logger.info(f"Listening on Kafka with {DOWNLOAD_WORKERS} download threads...")
        for message in self.consumer:
            if not self.is_running: break
            
            camera_data = message.value
            self.download_pool.submit(self.download_task, camera_data)

    def download_task(self, camera_data):
        """Task táº£i áº£nh vÃ  decode (Cháº¡y Ä‘a luá»“ng)"""
        try:
            liveview_url = camera_data.get('liveviewUrl')
            if not liveview_url: return

            image_bytes = self.image_fetcher.fetch_image(liveview_url)
            
            if image_bytes:
                nparr = np.frombuffer(image_bytes, np.uint8)
                image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                
                if image is not None:
                    self.inference_queue.put((image, camera_data))
        except Exception as e:
            logger.error(f"Download error: {e}")

    def gpu_inference_loop(self):
        """Luá»“ng GPU: Gom batch vÃ  cháº¡y YOLO"""
        logger.info(f" GPU Batch Processing Started (Batch Size: {BATCH_SIZE})")
        
        batch_images = []
        batch_metadata = []
        last_batch_time = time.time()

        while self.is_running:
            try:
                # Láº¥y áº£nh tá»« hÃ ng Ä‘á»£i
                # timeout ngáº¯n Ä‘á»ƒ check logic thá»i gian
                item = self.inference_queue.get(timeout=0.05)
                batch_images.append(item[0])
                batch_metadata.append(item[1])
            except Empty:
                pass

            # Äiá»u kiá»‡n cháº¡y Batch:
            # 1. Äá»§ sá»‘ lÆ°á»£ng (BATCH_SIZE)
            # 2. HOáº¶C ÄÃ£ Ä‘á»£i quÃ¡ lÃ¢u (BATCH_TIMEOUT) vÃ  cÃ³ Ã­t nháº¥t 1 áº£nh
            current_time = time.time()
            is_batch_full = len(batch_images) >= BATCH_SIZE
            is_timeout = (current_time - last_batch_time >= BATCH_TIMEOUT) and len(batch_images) > 0

            if is_batch_full or is_timeout:
                try:
                    # --- CHáº Y INFERENCE ---
                    # ÄÆ°a cáº£ list áº£nh vÃ o model 1 láº§n
                    results = self.yolo_model.analyze_image(batch_images)
                    
                    # GhÃ©p káº¿t quáº£ láº¡i vá»›i metadata
                    for i, result in enumerate(results):
                        meta = batch_metadata[i]
                        original_img = batch_images[i]
                        
                        # Äáº©y sang hÃ ng Ä‘á»£i xá»­ lÃ½ káº¿t quáº£
                        self.upload_queue.put((result, meta, original_img))
                    
                except Exception as e:
                    logger.error(f"Inference Batch Error: {e}")
                finally:
                    # Reset batch
                    batch_images = []
                    batch_metadata = []
                    last_batch_time = time.time()

    def result_processing_loop(self):
        """Luá»“ng láº¥y káº¿t quáº£ tá»« GPU -> Submit task upload"""
        logger.info(f"ğŸ“¤ Result processing started with {UPLOAD_WORKERS} upload threads...")
        while self.is_running:
            try:
                item = self.upload_queue.get(timeout=1)
                # item: (yolo_result, metadata, original_image_numpy)
                
                # Submit task upload vÃ o ThreadPool
                self.upload_pool.submit(self.upload_and_publish_task, item)
            except Empty:
                continue

    def upload_and_publish_task(self, item):
        """Task váº½ áº£nh, upload MinIO vÃ  báº¯n Kafka"""
        try:
            yolo_result, camera_data, image = item
            
            # 1. Äáº¿m Ä‘á»‘i tÆ°á»£ng
            count_result = self.yolo_model.count_objects([yolo_result]) # HÃ m nÃ y cáº§n chá»‰nh nháº¹ Ä‘á»ƒ nháº­n list hoáº·c 1 item
            
            # 2. Váº½ áº£nh (Chá»‰ váº½ náº¿u cáº§n lÆ°u - Tá»‘i Æ°u CPU)
            # annotated_image = yolo_result.plot()
            
            # 3. Upload MinIO (CÃ³ thá»ƒ bá» qua náº¿u khÃ´ng cáº§n thiáº¿t Ä‘á»ƒ tiáº¿t kiá»‡m I/O)
            # á» Ä‘Ã¢y tÃ´i giá»¯ láº¡i nhÆ°ng báº¡n nÃªn cÃ¢n nháº¯c chá»‰ upload khi cÃ³ sá»± kiá»‡n Ä‘áº·c biá»‡t
            annotated_image_url = None
            
            # Uncomment dÃ²ng dÆ°á»›i náº¿u muá»‘n upload áº£nh
            annotated_image = yolo_result.plot()
            _, buffer = cv2.imencode('.jpg', annotated_image)
            image_bytes = buffer.tobytes()
            annotated_image_url = self.minio_client.upload_image(
                 image_bytes,
                 camera_data.get('id'),
                 camera_data.get('timestamp')
            )

            # 4. Gá»­i Kafka Output
            self.publish_results(camera_data, count_result, annotated_image_url)

        except Exception as e:
            logger.error(f"Post-processing error: {e}")

    def publish_results(self, camera_data, count_result, annotated_image_url):
        # ... (Copy logic tá»« publish_results cÅ© vÃ o Ä‘Ã¢y)
        try:
            timestamp = camera_data.get('timestamp')
            vietnam_time = timestamp_to_vietnam_time(timestamp) if timestamp else None
            
            result = {
                'camera_id': camera_data.get('id'),
                'camera_name': camera_data.get('name'),
                'district': camera_data.get('dist'),
                'liveview_url': camera_data.get('liveviewUrl'),
                'coordinates': camera_data.get('loc', {}).get('coordinates', []),
                'total_count': count_result['total'],
                'detection_details': count_result['details'],
                'timestamp': timestamp,
                'timestamp_vn': vietnam_time,
                'annotated_image_url': annotated_image_url
            }
            
            self.producer.send(self.output_topic, result)
            # KhÃ´ng cáº§n flush liÃªn tá»¥c Ä‘á»ƒ tÄƒng tá»‘c, kafka tá»± batch
            # self.producer.flush() 
            
        except Exception as e:
            logger.error(f"Kafka publish error: {e}")